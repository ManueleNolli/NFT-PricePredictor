{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning project\n",
    "Author: Manuele Nolli, student BSc Computer Science SUPSI \n",
    "\n",
    "Date: 05.05.2023\n",
    "\n",
    "Mail: manuele.nolli@student.supsi.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This document is an analysis of the dataset \"Crypto Coven\" from __[Kaggle.com](https://www.kaggle.com/datasets/harrywang/crypto-coven)__. \n",
    "\n",
    "## Goal\n",
    "The goal of this project is to analyze the dataset and to create a model that can predict the price of a token based on the features of the token. \n",
    "\n",
    "In addition, the project will divide the tokens into different categories based on the price of the token and create a model that can predict the category of a token based on the features of the token.\n",
    "\n",
    "## Dataset description\n",
    "The dataset contains 9761 rows and 45 columns. The columns are:\n",
    "- **id**: unique identifier of the transaction\n",
    "- **num_sales**: number of sales of the token\n",
    "- **name**: name of the token\n",
    "- **description**: description of the token\n",
    "- **external_link**: link to the external website\n",
    "- **permalink**: link to the token on OpenSea\n",
    "- **token_metadata**: link to the metadata of the token\n",
    "- **token_id**: unique identifier of the token\n",
    "- **owner.user.username**: username of the owner\n",
    "- **owner.address**: address of the owner\n",
    "- **last_sale.total_price**: price of the last sale\n",
    "- **last_sale.payment_token.usd_price**: price of the last sale in USD\n",
    "- **last_sale.transaction.timestamp**: timestamp of the last sale\n",
    "- **Wonder**: Wonder of the token\n",
    "- **Skin Tone**: Skin Tone of the token\n",
    "- **Rising Sign**: Rising Sign of the token\n",
    "- **Eyebrows**: Eyebrows of the token\n",
    "- **Wisdom**: Wisdom of the token\n",
    "- **Body Shape**: Body Shape of the token\n",
    "- **Moon Sign**: Moon Sign of the token\n",
    "- **Will**: Will of the token\n",
    "- **Hair Color**: Hair Color of the token\n",
    "- **Wit**: Wit of the token\n",
    "- **Wiles**: Wiles of the token\n",
    "- **Necklace**: Necklace of the token\n",
    "- **Sun Sign**: Sun Sign of the token\n",
    "- **Eye Style**: Eye Style of the token\n",
    "- **Eye Color**: Eye Color of the token\n",
    "- **Mouth**: Mouth of the token\n",
    "- **Hat**: Hat of the token\n",
    "- **Archetype of Power**: Archetype of Power of the token\n",
    "- **Woe**: Woe of the token\n",
    "- **Hair (Front)**: Hair (Front) of the token\n",
    "- **Top**: Top of the token\n",
    "- **Hair (Back)**: Hair (Back) of the token\n",
    "- **Background**: Background of the token\n",
    "- **Face Markings**: Face Markings of the token\n",
    "- **Facewear**: Facewear of the token\n",
    "- **Hair Topper**: Hair Topper of the token\n",
    "- **Back Item**: Back Item of the token\n",
    "- **Earrings**: Earrings of the token\n",
    "- **Forehead Jewelry**: Forehead Jewelry of the token\n",
    "- **Hair (Middle)**: Hair (Middle) of the token\n",
    "- **Mask**: Mask of the token\n",
    "- **Outerwear**: Outerwear of the token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# read data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/witches.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# read data\n",
    "df = pd.read_csv('data/witches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show information about the features present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset info function\n",
    "def printDatasetInfo(df):\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(\"Columns names:\", end=\" \")\n",
    "    for col in df:\n",
    "            print(col, end=\", \")\n",
    "    print()\n",
    "    \n",
    "    #columns types\n",
    "    print(f\"Columns type:\")\n",
    "    #creating temp array\n",
    "    columnData = []\n",
    "    dfIndexType = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        temp = []\n",
    "        dfIndexType.append(col)\n",
    "        temp.append(df[col].apply(type).unique())\n",
    "        temp.append(df[col].isnull().sum())\n",
    "        temp.append(round((df[col].isnull().sum() / len(df[col])) * 100, 2))\n",
    "        temp.append(df[col].nunique())\n",
    "        columnData.append(temp)\n",
    "    \n",
    "    #create new Dataframe\n",
    "    dfColumnsType = pd.DataFrame(columnData, columns=['Types','NaN Count', 'NaN %','Unique Values'])\n",
    "    dfColumnsType.index = dfIndexType\n",
    "    #print columns type\n",
    "    display(dfColumnsType)\n",
    "\n",
    "# print dataset info\n",
    "printDatasetInfo(df)\n",
    "\n",
    "#df size\n",
    "print(f\"Dataframe rows: {len(df)}\")\n",
    "\n",
    "#df sample\n",
    "print(\"Dataset samples:\")\n",
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Converting features in Present / Not present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, it is notable that some witch's caracteristics are present only for a small number of tokens. For example, the \"Mask\" featurs has a 95% of null values. For simplicity, the features with more than 50% of null values will be considered as \"not present\" or \"present\" and not as a specific value.\n",
    "\n",
    "It is possible to see that the dataset has approximately 50% of the tokens with a price of 0. This means that the tokens have never been sold. \n",
    "The dataset will be divided into two datasets: one with the tokens that have never been sold and one with the tokens that have been sold at least once.\n",
    "\n",
    "The data preprocessing will continue with dropping the columns that are not useful for the analysis and the creation of other simpler columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove not useful columns: id, external link, token metadata, owner username and address, last_sale.transaction.timestamp\n",
    "df.drop(['id', 'external_link', 'token_metadata', 'owner.user.username', 'owner.address', 'last_sale.transaction.timestamp', 'Rising Sign', 'Moon Sign', 'Sun Sign'], axis=1, inplace=True)\n",
    "\n",
    "# create featureList with all features that have more than 50% of null values and another list with all features that have less than 0.5% of null values but not 0\n",
    "featureList50 = []\n",
    "featureList01 = []\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > len(df[col]) * 0.5:\n",
    "        featureList50.append(col)\n",
    "    elif df[col].isnull().sum() < len(df[col]) * 0.005 and df[col].isnull().sum() != 0:\n",
    "        featureList01.append(col)\n",
    "\n",
    "# create has_'Feature' column based on null values in 'feature' column\n",
    "for feature in featureList50:\n",
    "    df['has_' + feature] = df[feature].notnull()\n",
    "\n",
    "# drop features present in featureList\n",
    "df.drop(featureList50, axis=1, inplace=True)\n",
    "\n",
    "# drop null values in featureList01\n",
    "df.dropna(subset=featureList01, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into two groups: one with missing values of the variable 'last_sale.total_price' \n",
    "df_missingPrice = df[df['last_sale.total_price'].isnull()]\n",
    "df = df[df['last_sale.total_price'].notnull()]\n",
    "\n",
    "# Convert the 'last_sale.total_price' GWEI to ETH\n",
    "df['last_sale.total_price'] = df['last_sale.total_price'].apply(lambda x: x / 10 ** 18)\n",
    "# Create a new column with the USD price (In last_sale.total_price column there are ETH prices of the sales times)\n",
    "df['price_USD'] = df['last_sale.total_price'] * df['last_sale.payment_token.usd_price']\n",
    "\n",
    "#drop not useful columns\n",
    "df.drop(['last_sale.payment_token.usd_price', 'last_sale.total_price'], axis=1, inplace=True)\n",
    "\n",
    "# print shape of the two groups\n",
    "print(f\"Shape of the group with missing price values: {df_missingPrice.shape}\")\n",
    "print(f\"Shape of the group without missing price values: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Adapt numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset are present some numeric features with a score form 0 to 9. For simplicity, these features will be renamed to score_'Feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename score features: Wiles, Will, Wisdom, Wit, Woe, Wonder\n",
    "\n",
    "scoreFeature = ['Wiles', 'Will', 'Wisdom', 'Wit', 'Woe', 'Wonder']\n",
    "\n",
    "for i in range(len(scoreFeature)):\n",
    "    #Rename\n",
    "    df.rename(columns={f'{scoreFeature[i]}': f'score_{scoreFeature[i]}'}, inplace=True)\n",
    "\n",
    "# Create a copy of the dataframe for the hashing section\n",
    "df_hashing = df.copy()\n",
    "\n",
    "# Copy dataset for Classification\n",
    "df_class = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Reduce cardinality of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! The dataset is almost ready for the analysis. The last step is to reduce the cardinality of the categorical features. The goal is to reduce the cardinality to a maximum of 3 values. The best solution will be to reduce the cardinality to 2 values: \"Rare\" and \"Not Rare\". In addition, the features will be renamed to rarity_'Feature'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce cardinality: Skin Tone, Rising Sign, Eyebrows, Moon Sign, Hair Color, Eye Style, Eye Color, Mouth, Archetype of Power, Hair (Front), Top, Hair (Back), Background\n",
    "\n",
    "categoricalFeature = ['Skin Tone', 'Eyebrows', 'Hair Color', 'Eye Style', 'Eye Color', 'Mouth', 'Archetype of Power', 'Hair (Front)', 'Top', 'Hair (Back)', 'Background']\n",
    "\n",
    "# Reduce methodology: If a value count is in more than the third quartile of the total count, it will be replaced by 'Not Rare'. The others will be replaced by 'Rare'\n",
    "# In addition, if a value is null it will be replaced by 'Not Rare'\n",
    "for i in range(len(categoricalFeature)):\n",
    "    # Replace null values by 'Not Rare'\n",
    "    df[categoricalFeature[i]].fillna('Not Rare', inplace=True)\n",
    "\n",
    "    # Calculate value\n",
    "    sumCount = df[categoricalFeature[i]].value_counts().sum()\n",
    "    numUnique = df[categoricalFeature[i]].nunique()\n",
    "    thirdQuartile = sumCount/numUnique * 0.75\n",
    "\n",
    "    # Replace values by 'Rare' or 'Not Rare'\n",
    "    df[categoricalFeature[i]] = df[categoricalFeature[i]].apply(lambda x: 'Rare' if df[categoricalFeature[i]].value_counts()[x] < thirdQuartile else 'Not Rare')\n",
    "    \n",
    "\n",
    "    # Rename the column as 'rarity_' + column name\n",
    "    df.rename(columns={f'{categoricalFeature[i]}': f'rarity_{categoricalFeature[i]}'}, inplace=True)\n",
    "\n",
    "# If there are features that have only one value, they will be removed\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() == 1:\n",
    "        df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset info\n",
    "print(\"Final dataset info:\")\n",
    "printDatasetInfo(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to see from the table above, the dataset is now cleaned. We are ready to start the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data visualization\n",
    "In this section, it will be shown some graphs that can help to understand the dataset. \n",
    "\n",
    "A recap of the previous section:\n",
    "* the features with more than 50% of null values will be considered as \"not present\" or \"present\" and not as a specific value. Those features are called has_'Feature'\n",
    "* there are some features that present a score from 0 to 9. Those features are called score_'Feature'\n",
    "* the remaining features (with discrete values) are called rarity_'Feature' and their values are \"Rare\" or \"Not Rare\" depending of the third quartile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Price distribution\n",
    "The price distribution is shown in the graph below. The price is in USD. The graph shows that the majority of the tokens have a low price. In addition, the graph shows that the price is not normally distributed. The price is right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution\n",
    "fig = sns.displot(df['price_USD'], kde=True).set(title='Price distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Percentage distribution of the features prensence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of has_'Feature' columns plot \n",
    "# create temp array\n",
    "temp = []\n",
    "for col in df.columns:\n",
    "    if col.startswith('has_'):\n",
    "        temp.append([col,df[col].value_counts()[0]/len(df), df[col].value_counts()[1]/len(df)])\n",
    "\n",
    "# create new dataframe\n",
    "dfHasFeature = pd.DataFrame(temp, columns=['Name','False [%]', 'True [%]'])\n",
    "# Order item \n",
    "dfHasFeature.sort_values(by=['True [%]'], inplace=True)\n",
    "\n",
    "# Plot\n",
    "plt = dfHasFeature.plot(kind='barh', stacked=True, figsize=(20,10), title='Percentage of has_\\'Feature\\' columns', x='Name', width=0.8)\n",
    "plt.set_xlabel('Percentage')\n",
    "plt.set_ylabel('Feature')\n",
    "\n",
    "# Add more x ticks\n",
    "plt.set_xticks(np.arange(0, 1.05, 0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Percentage distribution of the features rarity\n",
    "It is possible to see that the previous calculation of the rarity of the features is correct. The majority of the features have a small percentage of \"Rare\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of rarity_'Feature' columns plot \n",
    "# create temp array\n",
    "temp = []\n",
    "for col in df.columns:\n",
    "    if col.startswith('rarity_'):\n",
    "        if(len(df[col].value_counts()) == 1):\n",
    "            if(df[col].value_counts().index[0] == 'Rare'):\n",
    "                temp.append([col,df[col].value_counts()[0]/len(df), 0])\n",
    "            else:\n",
    "                temp.append([col,0, df[col].value_counts()[0]/len(df)])\n",
    "        else:\n",
    "            temp.append([col,df[col].value_counts()[1]/len(df), df[col].value_counts()[0]/len(df)])\n",
    "\n",
    "# create new dataframe\n",
    "dfHasFeature = pd.DataFrame(temp, columns=['Name','Rare [%]', 'Not Rare [%]'])\n",
    "# Order item \n",
    "dfHasFeature.sort_values(by=['Rare [%]'], inplace=True)\n",
    "\n",
    "# Plot\n",
    "plt = dfHasFeature.plot(kind='barh', stacked=True, figsize=(20,10), title='Percentage of rarity_\\'Feature\\' columns', x='Name', width=0.8)\n",
    "plt.set_xlabel('Percentage')\n",
    "plt.set_ylabel('Feature')\n",
    "\n",
    "# Add more x ticks\n",
    "plt.set_xticks(np.arange(0, 1.05, 0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Distribution of 'score_' features\n",
    "\n",
    "# create temp array\n",
    "temp = []\n",
    "for col in df.columns:\n",
    "    if col.startswith('score_'):\n",
    "        temp.append([col,df[col].mean(), df[col].median(), df[col].mode()[0], df[col].std()])\n",
    "\n",
    "# Create subplots\n",
    "height = int(len(temp)/2)\n",
    "width = 2\n",
    "fig, axs = plt.subplots(height, width, figsize=(10*width, 10*height))\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    # Histogram\n",
    "    max_n = df[temp[i][0]].max()\n",
    "    min_n = df[temp[i][0]].min()\n",
    "    bins = 10\n",
    "    step = (max_n - min_n) / bins\n",
    "    arr_div = np.arange(min_n + step / 2, max_n + step / 2, step=step)\n",
    "    arr_div_r = np.round(arr_div, 0).astype(int)\n",
    "    fig = sns.histplot(data = df, x=temp[i][0], kde=True, bins=bins, ax=axs[i%height,i%width]).set(title=f'{temp[i][0]} distribution')\n",
    "    axs[i % height, i % width].set_xticks(arr_div)\n",
    "    axs[i % height, i % width].set_xticklabels(arr_div_r)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Correlation matrix\n",
    "As it is possible to see from the graph above, there is not a correlation between the features. This means that the features are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corrMatrix = df.corr(numeric_only=True)\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(corrMatrix, annot=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Price prediction model\n",
    "The goal of this section is to create a model that can predict the price of a token based on the features of the token.\n",
    "It will be done following these steps:\n",
    "1. Split the dataset into train, validation and test set (60%, 20%, 20%) using K-Fold cross validation\n",
    "2. LASSO regression with all the features. Some features will be removed from the model if they have a p-value > 0.05\n",
    "3. RIDGE regression with the features selected in the previous step\n",
    "\n",
    "**Note**: In this dataset is not needed to scale the features because the features are already scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Convertion of the features in numeric values and dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver binary columns to 0 and 1\n",
    "\n",
    "# Create a list of columns to be converted\n",
    "feature = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if col.startswith('has_') or col.startswith('rarity_'):\n",
    "        feature.append(col)\n",
    "\n",
    "# if present, replace 'True' with 1 and 'False' with 0. If rare, replace 'Rare' with 1 and 'Not Rare' with 0\n",
    "for col in feature:\n",
    "    if col.startswith('has_'):\n",
    "        df[col] = df[col].apply(lambda x: 1 if x == True else 0)\n",
    "    else:\n",
    "        df[col] = df[col].apply(lambda x: 1 if x == 'Rare' else 0)\n",
    "\n",
    "# It remains to convert the Body Shape feature with dummy variables\n",
    "df = pd.get_dummies(df, columns=['Body Shape'], prefix = ['dummies_BodyShape'])\n",
    "\n",
    "# Add to the list of features the dummy variables and the score features\n",
    "for col in df.columns:\n",
    "    if col.startswith('dummies_') or col.startswith('score_'):\n",
    "        feature.append(col)\n",
    "\n",
    "feature.append(\"num_sales\")\n",
    "# Print dataset info\n",
    "printDatasetInfo(df)\n",
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Split the dataset\n",
    "Split the dataset into train, validation and test set (60%, 20%, 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split dataset in train, validation and test set\n",
    "\n",
    "y = df['price_USD']\n",
    "X = df.drop(['price_USD'], axis=1)\n",
    "# Drop all columns not in the feature list\n",
    "X = X[feature]\n",
    "\n",
    "# Split dataset in train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# *Note*: The validation set is split in the following code section with the GridSearchCV\n",
    "# Split train set in train and validation set\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 LASSO regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings due to the GridSearchCV\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "from sklearn.metrics import  r2_score\n",
    "\n",
    "def printResults(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Lasso regression\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    'alpha': 10**np.linspace(10,-2,100)*0.5 ,\n",
    "    'max_iter': [1000, 10000, 100000],\n",
    "    'tol': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "crossValidation = 5\n",
    "lasso_regressor = GridSearchCV(estimator = lasso, param_grid = param_grid, cv = crossValidation, scoring='neg_mean_squared_error') \n",
    "\n",
    "# Fit the grid search to the data\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(f'Best params {lasso_regressor.best_params_}')\n",
    "print(f'Best score {-lasso_regressor.best_score_}')\n",
    "\n",
    "# Create a list of features with a coefficient different from 0\n",
    "feature_lasso = []\n",
    "for i in range(len(lasso_regressor.best_estimator_.coef_)):\n",
    "    if lasso_regressor.best_estimator_.coef_[i] != 0:\n",
    "        feature_lasso.append(feature[i])\n",
    "\n",
    "# Print the number of features\n",
    "print(f'Number of features: {len(feature_lasso)} that are important for the prediction')\n",
    "\n",
    "# Print the features\n",
    "print(f'Best features: {feature_lasso}')\n",
    "\n",
    "# Print the score on the test set\n",
    "printResults(lasso_regressor.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 RIDGE regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Ridge regression\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'max_iter': [1000, 10000, 100000],\n",
    "    'tol': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# Create new train and test set with only the important features\n",
    "X_train = X_train[feature_lasso]\n",
    "X_test = X_test[feature_lasso]\n",
    "\n",
    "# Create a based model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "crossValidation = 5\n",
    "ridge_regressor = GridSearchCV(estimator = ridge, param_grid = param_grid, cv = crossValidation, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "ridge_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(f'Best params {ridge_regressor.best_params_}')\n",
    "print(f'Best score {-ridge_regressor.best_score_}')\n",
    "\n",
    "# Print the score on the test set\n",
    "printResults(ridge_regressor.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Random Forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Random forest regression\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [None, 10, 50, 100],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create new train and test set with only the important features\n",
    "X_train = X_train[feature_lasso]\n",
    "X_test = X_test[feature_lasso]\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "crossValidation = 5\n",
    "rf_regressor = GridSearchCV(estimator = rf, param_grid = param_grid, cv = crossValidation, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(f'Best params {rf_regressor.best_params_}')\n",
    "print(f'Best score {-rf_regressor.best_score_}')\n",
    "\n",
    "# Print the score on the test set\n",
    "printResults(rf_regressor.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Hashing\n",
    "As is possible to see from the above models, the scorse is very bad, near 0. This is due to the fact that the features are categorical and not numerical. Maybe in the dataprocessing the features were extremely reduced and the model is not able to predict the price of the token.\n",
    "\n",
    " In order to improve the score, it is possible to use the hashing trick. The hashing trick is a method that converts categorical features into numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows with missing values for Eyebrows and Hair Color (A max of 176 rows are deleted)\n",
    "df_hashing = df_hashing.dropna(subset=['Eyebrows', 'Hair Color'])\n",
    "\n",
    "# Convert Hair Color NaN values to 'Unknown'\n",
    "df_hashing['Hair Color'] = df_hashing['Hair Color'].fillna('Unknown')\n",
    "df_hashing['Hair (Back)'] = df_hashing['Hair (Back)'].fillna('Unknown')\n",
    "\n",
    "# Drop unnecessary columns: num_sales, name, description, permalink, token_id\n",
    "df_hashing = df_hashing.drop([ 'name', 'description', 'permalink', 'token_id'], axis=1)\n",
    "\n",
    "for col in feature:\n",
    "    if col.startswith('has_'):\n",
    "        df[col] = df[col].apply(lambda x: 1 if x == True else 0)\n",
    "    else:\n",
    "        df[col] = df[col].apply(lambda x: 1 if x == 'Rare' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hashing\n",
    "from category_encoders import HashingEncoder\n",
    "\n",
    "hashingFeatures = []\n",
    "\n",
    " # All features that have not \"has_\", \"score_\" in the name\n",
    "for feature in df_hashing.columns:\n",
    "    if 'has_' not in feature and 'score_' not in feature and 'price_USD' not in feature:\n",
    "        hashingFeatures.append(feature)\n",
    "\n",
    "y = df_hashing['price_USD']\n",
    "X = df_hashing.drop(['price_USD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a helper function to get the scores for each encoding method:\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def get_score(model, X, y, X_val, y_val):\n",
    "    model.fit(X, y)\n",
    "    # Calculate the r2 score\n",
    "    r2 = model.score(X_val, y_val)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Split dataset into train and validation subsets:\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y , test_size=0.2, random_state=1)\n",
    "\n",
    "# Lasso regression\n",
    "\n",
    "# Iterate over different n_components:\n",
    "for n_components in [10, 100, 500, 1000, 5000, 10000]:\n",
    "    \n",
    "    hashing_enc = HashingEncoder(cols=hashingFeatures, n_components=n_components).fit(X_train, y_train)\n",
    "    \n",
    "    X_train_hashing = hashing_enc.transform(X_train.reset_index(drop=True))\n",
    "    X_val_hashing = hashing_enc.transform(X_val.reset_index(drop=True))\n",
    "\n",
    "    # # Create a based model\n",
    "    lasso = Lasso()\n",
    "\n",
    "    # print the scores for each encoding method\n",
    "    score = get_score(lasso, X_train_hashing, y_train, X_val_hashing, y_val)\n",
    "    print(\"Lasso regression score with n_components = {}: {}\".format(n_components, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Conclusion\n",
    "Unfortunately, all the models have a very low score. Although the hashing trick, the score is not improved. \n",
    "A possible reason could be that NFT are art and the price is not related to the features of the token. The price is related to the artist and the popularity of the artist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification model\n",
    "In this section the price will be divided into 3 classes:\n",
    "* Low price: price < 1000\n",
    "* Medium price: 1000 <= price < 10000\n",
    "* High price: price >= 10000\n",
    "\n",
    "The goal of this section is to create a model that can predict the price class of a token based on the features of the token.\n",
    "It will be done using UMAP for show a possible clustering of the tokens and then using different classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve function that will be used to plot the ROC curve for model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def plot_roc_curve(y_test, y_pred):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    n_classes = 3\n",
    "\n",
    "    y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
    "    y_pred_bin = label_binarize(y_pred, classes=[0, 1, 2])\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot of a ROC curve for a specific class\n",
    "    for i in range(n_classes):\n",
    "        plt.figure()\n",
    "        plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([-0.05, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve for class {i}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate and plot confusion matrix, it will be used for each model\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, classes, title=None):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp = disp.plot(include_values=True, cmap=plt.cm.Blues, ax=None, xticks_rotation='horizontal')\n",
    "    if title is not None:\n",
    "        disp.ax_.set_title(title)\n",
    "    # Calculate TP, FP, FN, and TN for each class\n",
    "    for i in range(len(cm)):\n",
    "        tp = cm[i,i]\n",
    "        fp = np.sum(cm[:,i]) - tp\n",
    "        fn = np.sum(cm[i,:]) - tp\n",
    "        tn = np.sum(cm) - tp - fp - fn\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        \n",
    "        print(f\"Class {i} -- TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}, Sensitivity: {sensitivity}, Specificity: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing for classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Drop unnecessary columns: num_sales, name, description, permalink, token_id\n",
    "df_class = df_class.drop(['name', 'description', 'permalink'], axis=1)\n",
    "\n",
    "# Create a new column with price (LOW, MEDIUM, HIGH)\n",
    "df_class['price_USD'] = df_class['price_USD'].astype(float)\n",
    "df_class['price_USD'] = df_class['price_USD'].apply(lambda x: 'LOW' if x < 1000 else 'MEDIUM' if x < 5000 else 'HIGH')\n",
    "\n",
    "# print the unique values of the column and their counts\n",
    "print(\"Counts for price_USD:\")\n",
    "print(df_class['price_USD'].value_counts())\n",
    "\n",
    "# Convert Hair Color NaN with SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "df_class['Hair (Back)'] = imputer.fit_transform(df_class[['Hair (Back)']])\n",
    "df_class['Hair Color'] = imputer.fit_transform(df_class[['Hair Color']])\n",
    "df_class['Eyebrows'] = imputer.fit_transform(df_class[['Eyebrows']])\n",
    "\n",
    "# Create a label encoder object\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "#Convert all df_class columns to numeric\n",
    "for col in df_class.columns:\n",
    "    df_class[col] = label_encoder.fit_transform(df_class[col])\n",
    "\n",
    "# separate target from predictors\n",
    "y = df_class['price_USD']\n",
    "X = df_class.drop(['price_USD'], axis=1)\n",
    "X = X.drop(['token_id'], axis=1)\n",
    "\n",
    "# Split dataset in train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 UMAP\n",
    "UMAP is a dimensionality reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP \n",
    "import umap \n",
    "\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "embedding = reducer.fit_transform(X_train)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image embedding for show in the plot\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "def embeddable_image(data): # data is the path to the image\n",
    "    img = Image.open(data)\n",
    "    img.thumbnail((64,64), Image.LANCZOS)\n",
    "    buffer = BytesIO()\n",
    "    img.save(buffer, format='png')\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show images in UMAP from ./data/images/$(token_id).png\n",
    "df_class['image'] = df_class[\"token_id\"].apply(lambda x: f\"./data/images/{x}.png\")\n",
    "\n",
    "# drop token_id = 0\n",
    "df_class = df_class.drop(df_class[df_class['token_id'] == 0].index)\n",
    "\n",
    "def draw_umap(n_neighbors=15, min_dist=0.1, metric='euclidean', title='', data=df_class):\n",
    "\n",
    "    fit = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=2,\n",
    "        metric=metric,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    data2 = data.drop(['image'], axis=1)\n",
    "    u = fit.fit_transform(data2)\n",
    "    source = ColumnDataSource(data=dict(\n",
    "        x=u[:,0],\n",
    "        y=u[:,1],\n",
    "        image=[embeddable_image(d) for d in data.image],\n",
    "        token_id=df[\"token_id\"],\n",
    "        name=df[\"name\"],\n",
    "        description=df[\"description\"],\n",
    "        permalink=df[\"permalink\"],\n",
    "        price=df[\"price_USD\"]\n",
    "    ))\n",
    "\n",
    "    plot_figure = figure(\n",
    "        title=title,\n",
    "        width=600,\n",
    "        height=600,\n",
    "        tools=('pan, wheel_zoom, reset')\n",
    "    )\n",
    "\n",
    "    plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "    <div>\n",
    "        <div>\n",
    "            <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    plot_figure.circle(\n",
    "        'x',\n",
    "        'y',\n",
    "        source=source,\n",
    "        line_alpha=0.6,\n",
    "        fill_alpha=0.6,\n",
    "        size=4\n",
    "    )\n",
    "\n",
    "    show(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (2, 5, 7, 10): # n_neighbors # 2, 5, 10, 20, 50, 100, 200\n",
    "    for d in ( 0.01, 0.1, 0.5): # min_dist 0.1, 0.25, 0.5, 0.75, 0.99\n",
    "        for m in ('euclidean', 'cosine', 'manhattan', 'correlation'):\n",
    "            draw_umap(n_neighbors=n, min_dist=d, metric=m, title='n_neighbors = {}, min_dist = {}, metric = {}'.format(n, d, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to see from the graph above, the tokens sometimes are clustered in groups, but not following the price. This means that there is a correlation between the features, but not with the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 KKN\n",
    "The first model that will be used is the KKN. The KKN is a classification model that classifies a new data point based on the k nearest data points. The k is a parameter that can be tuned.\n",
    "The model will be trained with grid search cross validation. \n",
    "KKN will be fit with 40 different values of k (from 1 to 40) and the best value of k will be selected.\n",
    "Unfortunately, the score is very low. This means that the KKN is not able to classify the tokens based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "error = []\n",
    "\n",
    "# Split dataset in train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Calculating error for K values between 1 and 40\n",
    "for i in range(1, 40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for KNN with n = 3\n",
    "# The best K value is 3\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, knn.classes_, title='Confusion matrix KNN n = 5')\n",
    "\n",
    "# print accuracy\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}' .format(knn.score(X_train, y_train)))\n",
    "\n",
    "# print roc curve\n",
    "plot_roc_curve(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another model that will be used is the Random Forest. The Random Forest is an ensemble model that uses multiple decision trees to classify the data. The model will be trained with grid search cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [100, 200, 300, 1000], 'max_features': [ 'sqrt', 'log2']}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, refit=True, verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "grid_predictions = grid.predict(X_test)\n",
    "\n",
    "#Print best parameters\n",
    "print(\"Best params:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Best estimator:\")\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# plot confusion matrix\n",
    "plot_confusion_matrix(y_test, grid_predictions, grid.classes_, title='Confusion matrix Random Forest')\n",
    "\n",
    "# print roc curve\n",
    "plot_roc_curve(y_test, grid_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model that will be used is the Decision Tree. The Decision Tree is a model that uses a tree structure to classify the data. The model will be trained with grid search cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, refit=True, verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "grid_predictions = grid.predict(X_test)\n",
    "\n",
    "#Print best parameters\n",
    "print(\"Best params:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Best estimator:\")\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# plot confusion matrix\n",
    "plot_confusion_matrix(y_test, grid_predictions, grid.classes_, title='Confusion matrix Random Forest')\n",
    "\n",
    "# print roc curve\n",
    "plot_roc_curve(y_test, grid_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "Unfortunately, all the models have a very low score. Although the hashing trick, the score is not improved.\n",
    "\n",
    "A possible reason could be that NFT are art and the price is not related to the features of the token. The price is related to the artist and the popularity of the artist.\n",
    "\n",
    "# 6. Future work\n",
    "\n",
    "In the future, it will be interesting to analyze the price of the NFTs based on the artist and the popularity of the artist. In addition, it will be interesting to analyze the price of the NFTs based on the category of the NFTs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9304250524a8a24d725ee403b7543d0168bed3bb4ceec1c28d8430e3bb61759e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
